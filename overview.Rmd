---
title: "Overview"
author: "Alex Holcombe"
date: "2/21/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

What Jackie and Matthew found with Vul's formula:

> We found that A values increase and V values decrease with practice. The problem was that neither the A, C, and V values evidenced learning of the consistent timing during the training phase. That is, the patterns did not differ between the conditions with fixed target positions and the condition with random target positions. We did find evidence for temporal learning at transfer for the early (position 4) and late (position 13) positions. We would be interested in seeing if your estimates would show clearer evidence of temporal learning at training and at transfer.

## Grouping the data

* Add vertical line where SPE=0.   'plotBunch' is what plots histograms, which calls geom_histogram

* be best to average the blocks for the first day, then average the blocks for the second day.  We would not be analyzing data for the third day.  This way, we are not crossing days when combining data. They clarified the grouping is Epochs 1-2 and 3-4 for the 3-Day condition.
Blocks 2-7 day one 
9-14 day two

* Restrict the modeling standard deviation to less than 3?  Check sigmas for other data to argue they shouldnt be that small. 

### Older notes
There are both "training" blocks, "practice" blocks (1,8,15, which I think were the first block of each day?), and "transfer" blocks (vary with day condition).

Ultimately want to analyze data broken down by:
* Group (determines in which serial positions the targets appeared)
* Condition (3-day, 2-day, 1-day)
* TargPos (target position this trial)
* Epochs (group of 3 Blocks). 

Jackie says: The epoch would be more stable than block. We arbitrarily chose to aggregate over 3 blocks, mostly because it was a common denominator of the total number of training blocks among the different levels of training. Each block contained 24 trials of a given target position in the conditions where target positions were fixed and 4 trials in the random conditionâ€”not enough in either case to get A, C, V estimates.

Analyzing that many would probably take all night. 

"Targets were in serial positions 3-14 for Group 3 in the training sessions.  Blocks 2-4 correspond to training for the 1-Day condition, 2-7 and 9-11 for the 2-Day condition, and blocks 2-7, 9-14, and 16-18 for the 3-Day condition (blocks 1, 8, and 15 were "practice blocks" of 10 or so trials).  So, Group 3 of the 3-Day condition would have the most data if you want target positions to be randomized.

We would also want to have the parameters for transfer for each target position (4, 7, 10, 13) separately from the training phase. This would be for the 1-, 2-, and 3-day conditions.

Finally, we would like to have parameters for the last epoch of training for each subject in the 1-, 2-, and 3-day conditions. An epoch is defined as 3 blocks.
"

## Complications for interpretation of the data:

* The participant is allowed to report a stimulus that was not presented
* The participant is allowed to report 0, meaning they saw no target

This means that any change in efficacy or the other parameters could be a result of changes in bias rather than any change in attention.

Eventually should do some sort of data analysis to look at false alarms to assess bias changes.

## The code

Why do I need to do the fitting twice? inspectHistsPart1.Rmd uses the raw data and calls *calc_curves_dataframe(.,minSPE,maxSPE,numItemsInStream)*, which has to estimate the parameters and create the psychometric curves. But surely I should only estimate the parameters once, rather than also doing it in analyzeAll.Rmd, which seems to also do that (by calling analyzeOneConditionDF for each data subset).